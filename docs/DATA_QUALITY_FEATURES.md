# Data Quality Features

## Overview

The Claims Risk Analyzer now includes robust data quality controls for handling common data issues in insurance claims datasets. This demonstrates our ETL and data governance capabilities to prospective clients.

## Features Added

### 1. Age Validation & Cleaning

**Problem**: Real-world datasets often contain invalid age values (negative numbers, impossibly high ages like 150+, data entry errors)

**Solution**: Configurable age validation with three handling strategies:
- **Impute mean**: Replace invalid ages with the mean of valid ages (rounded to nearest integer)
- **Exclude rows**: Remove rows with invalid ages from the analysis
- **Leave as-is**: Keep data unchanged but report issues for transparency

**Configuration**:
- Min valid age (default: 18)
- Max valid age (default: 90)

**Use Case**: For health insurance, age is critical for risk assessment. Invalid ages would skew actuarial models and premium calculations.

---

### 2. Income/Salary Validation & Cleaning

**Problem**: Income fields may have outliers, missing values, or data entry errors (e.g., $1 instead of $100,000)

**Solution**: Same three-strategy approach as age validation

**Configuration**:
- Min valid income (default: $10,000)
- Max valid income (default: $500,000)

**Use Case**: Income correlates with coverage levels and claim patterns. Clean income data improves underwriting accuracy.

---

### 3. Transparent Reporting

**Visual Feedback**: The app displays a "Data Quality Report" card showing:
- Which columns were cleaned
- How many values were affected
- What values were imputed (if applicable)
- Total rows removed (if exclusion was selected)

**Example Output**:
```
ðŸ”§ Age: 15 out-of-range value(s) imputed to mean = 42
ðŸ”§ Income: 8 out-of-range value(s) imputed to mean = $65,000
Total rows removed: 0 of 1,000 (0.0%)
```

---

## Technical Implementation

### Code Architecture

**[helpers.py](../helpers.py)**: Core data quality utilities
- `handle_numeric_outliers()`: Generic function for cleaning numeric columns
- `get_data_quality_summary()`: Generate quality metrics for audit trails
- Constants: `AGE_MIN_DEFAULT`, `AGE_MAX_DEFAULT`, `INCOME_MIN_DEFAULT`, `INCOME_MAX_DEFAULT`

**[app.py](../app.py)**: UI controls and workflow integration
- Sidebar expandable sections for Age and Income validation
- Real-time feedback on data cleaning operations
- Integrated into data pipeline (runs after `coalesce_columns()`)

### Key Functions

```python
def handle_numeric_outliers(
    df: pd.DataFrame,
    column: str,
    method: str = "impute_mean",
    min_val: float | None = None,
    max_val: float | None = None,
    round_to_int: bool = True
) -> tuple[pd.DataFrame, int, float | None]:
    """
    Returns:
        - Cleaned DataFrame
        - Number of affected rows
        - Imputed value (if method='impute_mean')
    """
```

---

## Business Value for Sales Pitch

### Why This Matters to Insurers

1. **Data Governance**: Shows we understand real-world data quality challenges
2. **Transparency**: Clients can see exactly what was cleaned and why
3. **Flexibility**: Three different strategies match different business policies
4. **Auditability**: All cleaning operations are logged and reversible
5. **Production-Ready**: Demonstrates ETL capabilities needed for enterprise deployment

### Demo Talking Points

**For the Technical Audience**:
- "Notice how we handle data quality issues upstream, before they affect your risk models"
- "You control the business rules - we provide the framework"
- "All cleaning operations are auditable and can be exported for compliance"

**For the Business Audience**:
- "Bad data leads to bad decisions. We surface data quality issues automatically"
- "Your team sets the rules - what's a valid age range for your policies?"
- "This prevents the classic 'garbage in, garbage out' problem"

### ROI Impact

- **Prevents model errors**: Clean data = accurate risk scores
- **Saves analyst time**: Automated cleaning vs. manual spreadsheet work
- **Reduces fraud leakage**: Outlier detection catches data anomalies that may indicate fraud
- **Compliance-ready**: Audit trails for data transformations

---

## Usage

### In the UI

1. Load your dataset (upload CSV or use sample)
2. Open sidebar > "Data Quality" section
3. Expand "Age Validation" or "Income Validation"
4. Choose handling method and set min/max ranges
5. The data is cleaned automatically
6. View the "Data Quality Report" card to see what was cleaned

### For Testing

```python
from helpers import handle_numeric_outliers
import pandas as pd

df = pd.DataFrame({'age': [25, 150, 30, -5, 45]})

# Impute mean for out-of-range values
df_clean, affected, imputed = handle_numeric_outliers(
    df, 'age', method='impute_mean', min_val=18, max_val=90
)
print(f"Fixed {affected} ages, used value {imputed}")
```

---

## Future Enhancements

**Short-term** (could add before demo):
- Data quality scoring (A-F grade based on % of issues)
- Export data quality report to PDF/Excel
- Column-level quality metrics dashboard

**Long-term** (production features):
- Machine learning-based outlier detection for income
- Historical data quality trend tracking
- Integration with data catalogs (Alation, Collibra)
- Automated alerting when data quality drops below threshold
- Custom validation rules via config file

---

## Testing

To verify the installation:

```bash
# Activate virtual environment
source .venv/bin/activate

# Test the utilities
python3 -c "
from helpers import handle_numeric_outliers
import pandas as pd

df = pd.DataFrame({'age': [25, 150, 30, -5, 45]})
df_clean, affected, value = handle_numeric_outliers(
    df, 'age', method='impute_mean', min_val=18, max_val=90
)
print(f'âœ… Cleaned {affected} rows, imputed value: {value}')
"

# Run the app
streamlit run app.py
```

---

## Files Modified

- [helpers.py](../helpers.py): Added 3 new functions (~100 lines)
- [app.py](../app.py): Added sidebar controls and reporting (~80 lines)
- [update_quality_toggles.py](../update_quality_toggles.py): Installation script

**Backups created**:
- `helpers_YYYYMMDD_HHMMSS.bak`
- `app_YYYYMMDD_HHMMSS.bak`

---

## Security & Privacy Notes

**Data Handling**:
- All processing happens in-memory
- No data is sent to external services
- Cleaning operations are reversible (reload original CSV)

**Production Considerations**:
- Add logging for all data transformations
- Store cleaning configurations in audit database
- Implement role-based access for data quality rules
- Version control for data quality policies

---

*Created: 2025-10-13*
*Last Updated: 2025-10-13*
